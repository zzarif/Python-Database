{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline\n",
    "- Design model (define input / output size, define `forward()` pass)\n",
    "- Construct `loss` and `optimizer()`\n",
    "- Training loop:\n",
    "  - `forward()` pass: compute prediction\n",
    "  - calculate loss from predicted output\n",
    "  - `backward()` pass: compute graidents\n",
    "  - `optimize()`: update weights\n",
    "\n",
    "We are gonna do every single thing with PyTorch this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input / output has to be 2D array.\n",
    "\n",
    "Each **row** represents the number of **samples**.\n",
    "\n",
    "Each **column** represents the number of **features** in each sample.\n",
    "\n",
    "In the following case, there are four samples in `X` and each of the samples have only one feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set\n",
    "X_train = torch.tensor([[1],\n",
    "                        [2], \n",
    "                        [3], \n",
    "                        [4]], dtype=torch.float32)\n",
    "\n",
    "Y_train = torch.tensor([[2],\n",
    "                        [4], \n",
    "                        [6], \n",
    "                        [8]], dtype=torch.float32)\n",
    "\n",
    "# test set\n",
    "X_test = torch.tensor([5], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples, n_features = X_train.shape\n",
    "n_samples, n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we don't define `w` to a random value ourselves. We let PyTorch do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Design Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "model = nn.Linear(input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 1.661\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prediction before training: f(5) = {model(X_test).item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Construct `loss` and `optimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w = 0.669, loss = 22.22612\n",
      "epoch 2: w = 0.884, loss = 15.42308\n",
      "epoch 3: w = 1.063, loss = 10.70259\n",
      "epoch 4: w = 1.213, loss = 7.42714\n",
      "epoch 5: w = 1.337, loss = 5.15437\n",
      "epoch 6: w = 1.441, loss = 3.57733\n",
      "epoch 7: w = 1.527, loss = 2.48306\n",
      "epoch 8: w = 1.599, loss = 1.72376\n",
      "epoch 9: w = 1.659, loss = 1.19689\n",
      "epoch 10: w = 1.709, loss = 0.83130\n",
      "epoch 11: w = 1.750, loss = 0.57763\n",
      "epoch 12: w = 1.785, loss = 0.40160\n",
      "epoch 13: w = 1.814, loss = 0.27946\n",
      "epoch 14: w = 1.838, loss = 0.19470\n",
      "epoch 15: w = 1.858, loss = 0.13588\n",
      "epoch 16: w = 1.875, loss = 0.09506\n",
      "epoch 17: w = 1.889, loss = 0.06674\n",
      "epoch 18: w = 1.901, loss = 0.04708\n",
      "epoch 19: w = 1.910, loss = 0.03343\n",
      "epoch 20: w = 1.918, loss = 0.02396\n",
      "epoch 21: w = 1.925, loss = 0.01738\n",
      "epoch 22: w = 1.931, loss = 0.01281\n",
      "epoch 23: w = 1.936, loss = 0.00963\n",
      "epoch 24: w = 1.940, loss = 0.00743\n",
      "epoch 25: w = 1.943, loss = 0.00589\n",
      "epoch 26: w = 1.946, loss = 0.00482\n",
      "epoch 27: w = 1.948, loss = 0.00407\n",
      "epoch 28: w = 1.950, loss = 0.00355\n",
      "epoch 29: w = 1.952, loss = 0.00318\n",
      "epoch 30: w = 1.953, loss = 0.00293\n",
      "epoch 31: w = 1.954, loss = 0.00274\n",
      "epoch 32: w = 1.955, loss = 0.00261\n",
      "epoch 33: w = 1.956, loss = 0.00251\n",
      "epoch 34: w = 1.957, loss = 0.00244\n",
      "epoch 35: w = 1.958, loss = 0.00239\n",
      "epoch 36: w = 1.958, loss = 0.00235\n",
      "epoch 37: w = 1.959, loss = 0.00232\n",
      "epoch 38: w = 1.959, loss = 0.00229\n",
      "epoch 39: w = 1.959, loss = 0.00227\n",
      "epoch 40: w = 1.960, loss = 0.00225\n",
      "epoch 41: w = 1.960, loss = 0.00223\n",
      "epoch 42: w = 1.960, loss = 0.00221\n",
      "epoch 43: w = 1.961, loss = 0.00220\n",
      "epoch 44: w = 1.961, loss = 0.00218\n",
      "epoch 45: w = 1.961, loss = 0.00217\n",
      "epoch 46: w = 1.961, loss = 0.00216\n",
      "epoch 47: w = 1.961, loss = 0.00214\n",
      "epoch 48: w = 1.961, loss = 0.00213\n",
      "epoch 49: w = 1.962, loss = 0.00212\n",
      "epoch 50: w = 1.962, loss = 0.00210\n",
      "epoch 51: w = 1.962, loss = 0.00209\n",
      "epoch 52: w = 1.962, loss = 0.00208\n",
      "epoch 53: w = 1.962, loss = 0.00207\n",
      "epoch 54: w = 1.962, loss = 0.00205\n",
      "epoch 55: w = 1.962, loss = 0.00204\n",
      "epoch 56: w = 1.963, loss = 0.00203\n",
      "epoch 57: w = 1.963, loss = 0.00202\n",
      "epoch 58: w = 1.963, loss = 0.00200\n",
      "epoch 59: w = 1.963, loss = 0.00199\n",
      "epoch 60: w = 1.963, loss = 0.00198\n",
      "epoch 61: w = 1.963, loss = 0.00197\n",
      "epoch 62: w = 1.963, loss = 0.00196\n",
      "epoch 63: w = 1.963, loss = 0.00195\n",
      "epoch 64: w = 1.964, loss = 0.00193\n",
      "epoch 65: w = 1.964, loss = 0.00192\n",
      "epoch 66: w = 1.964, loss = 0.00191\n",
      "epoch 67: w = 1.964, loss = 0.00190\n",
      "epoch 68: w = 1.964, loss = 0.00189\n",
      "epoch 69: w = 1.964, loss = 0.00188\n",
      "epoch 70: w = 1.964, loss = 0.00187\n",
      "epoch 71: w = 1.964, loss = 0.00185\n",
      "epoch 72: w = 1.964, loss = 0.00184\n",
      "epoch 73: w = 1.964, loss = 0.00183\n",
      "epoch 74: w = 1.965, loss = 0.00182\n",
      "epoch 75: w = 1.965, loss = 0.00181\n",
      "epoch 76: w = 1.965, loss = 0.00180\n",
      "epoch 77: w = 1.965, loss = 0.00179\n",
      "epoch 78: w = 1.965, loss = 0.00178\n",
      "epoch 79: w = 1.965, loss = 0.00177\n",
      "epoch 80: w = 1.965, loss = 0.00176\n",
      "epoch 81: w = 1.965, loss = 0.00175\n",
      "epoch 82: w = 1.965, loss = 0.00174\n",
      "epoch 83: w = 1.966, loss = 0.00173\n",
      "epoch 84: w = 1.966, loss = 0.00171\n",
      "epoch 85: w = 1.966, loss = 0.00170\n",
      "epoch 86: w = 1.966, loss = 0.00169\n",
      "epoch 87: w = 1.966, loss = 0.00168\n",
      "epoch 88: w = 1.966, loss = 0.00167\n",
      "epoch 89: w = 1.966, loss = 0.00166\n",
      "epoch 90: w = 1.966, loss = 0.00165\n",
      "epoch 91: w = 1.966, loss = 0.00164\n",
      "epoch 92: w = 1.966, loss = 0.00163\n",
      "epoch 93: w = 1.967, loss = 0.00162\n",
      "epoch 94: w = 1.967, loss = 0.00162\n",
      "epoch 95: w = 1.967, loss = 0.00161\n",
      "epoch 96: w = 1.967, loss = 0.00160\n",
      "epoch 97: w = 1.967, loss = 0.00159\n",
      "epoch 98: w = 1.967, loss = 0.00158\n",
      "epoch 99: w = 1.967, loss = 0.00157\n",
      "epoch 100: w = 1.967, loss = 0.00156\n",
      "Prediction after training: f(5) = 9.933\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # forward pass to calculate y_pred\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    # calculate loss\n",
    "    l = loss(Y_train, y_pred)\n",
    "\n",
    "    # backward pass (calculate gradients)\n",
    "    l.backward()\n",
    "\n",
    "    # optimize params\n",
    "    optimizer.step()\n",
    "\n",
    "    # reset the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # print the params\n",
    "    [w, b] = model.parameters()\n",
    "    print(f\"epoch {epoch + 1}: w = {w[0][0].item():.3f}, loss = {l:.5f}\")\n",
    "\n",
    "print(f\"Prediction after training: f(5) = {model(X_test).item():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
